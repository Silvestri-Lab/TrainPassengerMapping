{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/vscode/.local/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/vscode/.local/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[K     |████████████████████████████████| 345 kB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.22.4\n",
      "  Downloading numpy-2.0.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 80.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[K     |████████████████████████████████| 505 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, numpy, pandas\n",
      "Successfully installed numpy-2.0.0 pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install pyspark\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/02 08:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, ArrayType\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import random \n",
    "import datetime\n",
    "\n",
    "# set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Railways Traffic Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.session.timeZone', 'CET') \n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True) \n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 10) \n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting trains for Suffix Tree algorithm building phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the csv file\n",
    "df = spark.read.csv('suffix_tree_input.csv', header=True) #0\n",
    "window_train = Window.partitionBy('codice_treno').orderBy('arrivo_teorico')\n",
    "\n",
    "# preproccesing for the train \n",
    "df_cleaned_trains = (\n",
    "    df\n",
    "    .withColumn('arrivo_teorico', f.col('arrivo_teorico').cast('int')).withColumn('partenza_teorica', f.col('partenza_teorica').cast('int'))\n",
    "    .withColumn('arrivo_teorico', f.when(f.col('arrivo_teorico') == 0, f.col('partenza_teorica')).otherwise(f.col('arrivo_teorico')))\n",
    "    .withColumn('partenza_teorica', f.when(f.col('partenza_teorica') == 0, f.col('arrivo_teorico')).otherwise(f.col('partenza_teorica')))\n",
    "    .withColumn('od_date_time_start', f.first('partenza_teorica').over(window_train))\n",
    "    .withColumn('second_value', f.lead('partenza_teorica', 1).over(window_train))\n",
    "    .withColumn('origin', f.first('station_id').over(window_train))\n",
    "    .withColumn('destination', f.last('station_id').over(window_train))\n",
    "    .select('codice_treno', 'od_date_time_start', 'second_value', 'origin', 'destination', 'stop_order')\n",
    "    .filter(f.col('stop_order') == 1)\n",
    "    .na.drop() \n",
    "    .distinct()\n",
    "    .groupBy('od_date_time_start', 'second_value','origin', 'destination')\n",
    "    .agg(f.collect_list('codice_treno').alias('codice_treno'))\n",
    "    .filter(f.col('od_date_time_start') != 0)\n",
    "    .withColumn('codice_treno', f.col('codice_treno')[0])\n",
    "    .select('codice_treno')\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('arrivo_teorico', f.col('arrivo_teorico').cast('int')).withColumn('partenza_teorica', f.col('partenza_teorica').cast('int'))\n",
    "    .withColumn('arrivo_teorico', f.when(f.col('arrivo_teorico') == 0, f.col('partenza_teorica')).otherwise(f.col('arrivo_teorico')))\n",
    "    .withColumn('partenza_teorica', f.when(f.col('partenza_teorica') == 0, f.col('arrivo_teorico')).otherwise(f.col('partenza_teorica')))\n",
    "    .join(df_cleaned_trains, on=['codice_treno'], how='inner') # clean identical trains with different train ids \n",
    "    .select('categoria', 'codice_treno', 'station_id', 'arrivo_teorico', 'partenza_teorica', 'stop_order', 'stazione')\n",
    "    .dropDuplicates()\n",
    "    .orderBy('codice_treno', 'arrivo_teorico')\n",
    "\n",
    "\n",
    ")\n",
    "df.toPandas().to_csv('experiment_input_train_stt_synthetic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users without train transfer PERFECT USERS and GROUND TRUTH generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "window_train = Window.partitionBy('codice_treno')\n",
    "\n",
    "mean = 10\n",
    "stddev = 10\n",
    "\n",
    "#1\n",
    "df_train_with_delay = (\n",
    "    df\n",
    "    # find the maximum stop_order for inside the window\n",
    "    .withColumn('stop_count', f.max('stop_order').over(window_train))\n",
    "    .select('categoria', 'codice_treno', 'stop_count')\n",
    "    .distinct()\n",
    "    # Generate a column 'delay' from a normal distribution with mean 0 and std 1\n",
    "    .withColumn('delay', f.randn() * stddev + mean)\n",
    "    .withColumn('delay', f.when(f.col('delay') < 0, 0).when(f.col('delay') > 60, 60).otherwise(f.col('delay')))\n",
    ")\n",
    "\n",
    "n = 1\n",
    "#2\n",
    "df_user_od = (\n",
    "    df_train_with_delay\n",
    "    .select('codice_treno', 'delay','stop_count')\n",
    "    # duplicate each column n\n",
    "    .withColumn('dummy', f.explode(f.array([f.lit(x) for x in range(n)])))\n",
    "    .drop('dummy')\n",
    "    .withColumn('IMSI', (f.rand(seed=42)*10000).cast('int'))\n",
    "    # with origin select a number between 0 and stop_count-2\n",
    "    .withColumn('origin', f.floor(f.rand(seed=42) * (f.col('stop_count') - 1)))\n",
    "    .withColumn('origin', f.when(f.col('origin') < 1, 1).otherwise(f.col('origin')))\n",
    "    # with destination select a number between origin+1 and stop_count-1\n",
    "    .withColumn('destination', f.col('origin') + 1 + f.floor(f.rand(seed=42) * (f.col('stop_count') - f.col('origin') - 1)))\n",
    "    .select('IMSI', 'codice_treno', 'origin', 'destination', 'delay')\n",
    "    .withColumnRenamed('origin', 'origin_idx')\n",
    "    .withColumnRenamed('destination', 'destination_idx')\n",
    ")\n",
    "\n",
    "window_user = Window.partitionBy('IMSI','codice_treno')\n",
    "#3 \n",
    "ground_truth = (\n",
    "    df_user_od\n",
    "    .join(df, on='codice_treno', how='inner')\n",
    "    # select only the rows between origin_idx and destination_idx in thw window_user\n",
    "    #.withColumn('stop_order', f.row_number().over(window_user))\n",
    "    .filter(f.col('stop_order') >= f.col('origin_idx'))\n",
    "    .filter(f.col('stop_order') <= f.col('destination_idx'))\n",
    "    .withColumn('arrivo_teorico', f.col('arrivo_teorico').cast('int'))\n",
    "    .withColumn('partenza_teorica', f.col('partenza_teorica').cast('int'))\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order')\n",
    "    .withColumn('origin_name', f.first('stazione').over(window_user))\n",
    "    .withColumn('destination_name', f.last('stazione').over(window_user))\n",
    "    .withColumn('origin', f.first('station_id').over(window_user))\n",
    "    .withColumn('destination', f.last('station_id').over(window_user))\n",
    "    .withColumn('od_date_time_start', f.first('partenza_teorica').over(window_user))\n",
    "    .withColumn('od_date_time_end', f.last('arrivo_teorico').over(window_user))\n",
    "    .distinct()\n",
    "    .filter(f.col('origin') != f.col('destination'))\n",
    "    .filter(f.col('od_date_time_start') < f.col('od_date_time_end'))\n",
    ")\n",
    "\n",
    "ground_truth.toPandas().to_csv('experiment_synthetic_user_ground_truth.csv', index=False) \n",
    "# create a lookup table for the train_id and the stat\n",
    "\n",
    "sytnethic_user_input_stt = (\n",
    "    ground_truth\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order', 'date_time_start')\n",
    "    .groupBy('DATE_ID', 'IMSI', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    # if the origini is different from the first position of the list of stations and the destination has to be equal to the last position of the list of stations\n",
    "    .filter((f.col('ORIGIN') == f.col('STATIONS')[0]) & (f.col('DESTINATION') == f.col('STATIONS')[f.size(f.col('STATIONS')) - 1]))\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['IMSI', 'ORIGIN', 'DESTINATION']) \n",
    "    .limit(10000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users without train transfer TEMPORAL NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth.csv', header=True)\n",
    "\n",
    "window_noiser = Window.partitionBy('IMSI','codice_treno').orderBy(f.rand())\n",
    "\n",
    "sytnethic_user_input_stt = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order', 'date_time_start')\n",
    "    # cast dealy column to int \n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    # add dealay to the date_time_start, dealys is in minutes and date_time_start is in seconds\n",
    "    .withColumn('DATE_TIME_START', f.col('DATE_TIME_START') + f.col('delay') * 60)\n",
    "    .withColumn('DATE_TIME_END', f.col('DATE_TIME_END') + f.col('delay') * 60)\n",
    "    .withColumn('DATE_TIME_START', f.col('DATE_TIME_START').cast('int'))\n",
    "    .withColumn('DATE_TIME_END', f.col('DATE_TIME_END').cast('int'))\n",
    "    .groupBy('DATE_ID', 'IMSI', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    # if the origini is different from the first position of the list of stations and the destination has to be equal to the last position of the list of stations\n",
    "    .filter((f.col('ORIGIN') == f.col('STATIONS')[0]) & (f.col('DESTINATION') == f.col('STATIONS')[f.size(f.col('STATIONS')) - 1]))\n",
    "    # drop a random STATIONS in the list, only for stations with more than 2 stations, it doens't have to be the first or the last in the station list\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['IMSI', 'ORIGIN', 'DESTINATION']) \n",
    "    .limit(10000)\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic_noise_only_time.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users without train transfer SPATIAL NOISE (deleting stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth.csv', header=True)\n",
    "\n",
    "sytnethic_user_input_stt = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order', 'date_time_start')\n",
    "    # cast delay column to int \n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    # add delay to the date_time_start, dealys is in minutes and date_time_start is in seconds\n",
    "    .groupBy('DATE_ID', 'IMSI', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    # if the origini is different from the first position of the list of stations and the destination has to be equal to the last position of the list of stations\n",
    "    .filter((f.col('ORIGIN') == f.col('STATIONS')[0]) & (f.col('DESTINATION') == f.col('STATIONS')[f.size(f.col('STATIONS')) - 1]))\n",
    "    # drop a random STATIONS in the list, only for stations with more than 2 stations, it doens't have to be the first or the last in the station list\n",
    "    .withColumn('STATIONS', f.when(f.size(f.col('STATIONS')) > 2, f.expr('transform(STATIONS, (x, i) -> if(i != 0 and i != size(STATIONS) - 1 and rand() > 0.5, null, x))')).otherwise(f.col('STATIONS')))\n",
    "    # find the index of the null values in the list of stations and drop the corresponding values in the list of dates\n",
    "    .withColumn('DATES_TIME_START', f.expr('transform(sequence(0, size(STATIONS) - 1), i -> if(STATIONS[i] is null, null, DATES_TIME_START[i]))'))\n",
    "    .withColumn('DATES_TIME_END', f.expr('transform(sequence(0, size(STATIONS) - 1), i -> if(STATIONS[i] is null, null, DATES_TIME_END[i]))'))\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['IMSI', 'ORIGIN', 'DESTINATION']) \n",
    "    .limit(10000)\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic_noise_only_spatial_(stations_removal).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users without train transfer SPATIAL NOISE (adding noise stations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the function to insert noises\n",
    "def insert_noise_station(stations, idx, noise_flag):\n",
    "    if noise_flag and idx != 0:\n",
    "        random_station = -7\n",
    "        stations.insert(idx, random_station)\n",
    "    return stations\n",
    "\n",
    "def insert_noise_time(time, idx, noise_flag):\n",
    "    if noise_flag and idx != 0:\n",
    "        time_to_insert = int((time[idx - 1] + time[idx]) / 2)\n",
    "        time.insert(idx, time_to_insert)\n",
    "    return time\n",
    "\n",
    "# Register the UDF\n",
    "insert_noise_station_udf = f.udf(insert_noise_station, ArrayType(IntegerType()))\n",
    "insert_noise_time_udf = f.udf(insert_noise_time, ArrayType(IntegerType()))\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth.csv', header=True)\n",
    "\n",
    "# Process the DataFrame\n",
    "sytnethic_user_input_stt = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order', 'DATE_TIME_START')\n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    .withColumn('DATE_TIME_START', (f.col('DATE_TIME_START') + f.col('delay') * 60).cast('int'))\n",
    "    .withColumn('DATE_TIME_END', (f.col('DATE_TIME_END') + f.col('delay') * 60).cast('int'))\n",
    "    .groupBy('DATE_ID', 'IMSI', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), \n",
    "         f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), \n",
    "         f.collect_list('DATE_TIME_END').alias('DATES_TIME_END')) \n",
    "    .withColumn('NOISE', f.rand() <= 0.5)\n",
    "    .withColumn('idx', f.floor(f.rand() * f.size(f.col('STATIONS'))).cast('int'))\n",
    "    # cast stations to array type of integer\n",
    "    .withColumn('STATIONS', f.col('STATIONS').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('STATIONS', insert_noise_station_udf(f.col('STATIONS'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('DATES_TIME_START', f.col('DATES_TIME_START').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('DATES_TIME_START', insert_noise_time_udf(f.col('DATES_TIME_START'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('DATES_TIME_END', f.col('DATES_TIME_END').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('DATES_TIME_END', insert_noise_time_udf(f.col('DATES_TIME_END'), f.col('idx'), f.col('NOISE'))) \n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .drop('NOISE', 'idx')\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# save the result\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic_noise_only_spatial_(stations_noise_add).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users without train transfer COMBINED NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth.csv', header=True)\n",
    "\n",
    "window_noiser = Window.partitionBy('IMSI','codice_treno').orderBy(f.rand())\n",
    "\n",
    "sytnethic_user_input_stt = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order', 'date_time_start')\n",
    "    # cast dealy column to int \n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    .withColumn('DATE_TIME_START', f.col('DATE_TIME_START') + f.col('delay') * 60)\n",
    "    .withColumn('DATE_TIME_END', f.col('DATE_TIME_END') + f.col('delay') * 60)\n",
    "    .withColumn('DATE_TIME_START', f.col('DATE_TIME_START').cast('int'))\n",
    "    .withColumn('DATE_TIME_END', f.col('DATE_TIME_END').cast('int'))\n",
    "    # add dealay to the date_time_start, dealys is in minutes and date_time_start is in seconds\n",
    "    .groupBy('DATE_ID', 'IMSI', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    # if the origini is different from the first position of the list of stations and the destination has to be equal to the last position of the list of stations\n",
    "    .filter((f.col('ORIGIN') == f.col('STATIONS')[0]) & (f.col('DESTINATION') == f.col('STATIONS')[f.size(f.col('STATIONS')) - 1]))\n",
    "    # drop a random STATIONS in the list, only for stations with more than 2 stations, it doens't have to be the first or the last in the station list\n",
    "    .withColumn('STATIONS', f.when(f.size(f.col('STATIONS')) > 2, f.expr('transform(STATIONS, (x, i) -> if(i != 0 and i != size(STATIONS) - 1 and rand() > 0.5, null, x))')).otherwise(f.col('STATIONS')))\n",
    "    # find the index of the null values in the list of stations and drop the corresponding values in the list of dates\n",
    "    .withColumn('DATES_TIME_START', f.expr('transform(sequence(0, size(STATIONS) - 1), i -> if(STATIONS[i] is null, null, DATES_TIME_START[i]))'))\n",
    "    .withColumn('DATES_TIME_END', f.expr('transform(sequence(0, size(STATIONS) - 1), i -> if(STATIONS[i] is null, null, DATES_TIME_END[i]))'))\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['IMSI', 'ORIGIN', 'DESTINATION']) \n",
    "    .limit(10000)\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic_noise_spatial_temporal.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users with train transfer PERFECT USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 09:24:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mean = 5\n",
    "stddev = 5\n",
    "# read the csv file\n",
    "synthetic_perfect_user_with_change_train = spark.read.csv('experiment_input_train_stt_synthetic.csv', header=True) #0\n",
    "window_train = Window.partitionBy('codice_treno').orderBy('arrivo_teorico')\n",
    "\n",
    "\n",
    "station_registry = spark.read.csv('experiment_station_registry_change_station.csv', header=True).filter(f.col('CHANGE_STATION') == True).select('STATION_ID')\n",
    "\n",
    "departure_trains = (\n",
    "    synthetic_perfect_user_with_change_train\n",
    "    .join(station_registry, on='STATION_ID', how='inner')\n",
    "    .filter(f.col('stop_order') == 1)\n",
    "    .withColumnRenamed('codice_treno', 'departure_train_id')\n",
    "    .withColumnRenamed('stop_order', 'departure_stop_order')\n",
    "    .withColumnRenamed('arrivo_teorico', 'departure_train_arrival_time')\n",
    "    .withColumnRenamed('partenza_teorica', 'departure_train_departure_time')\n",
    "    .select('STATION_ID', 'departure_train_id', 'departure_stop_order', 'departure_train_arrival_time', 'departure_train_departure_time')\n",
    ")\n",
    "\n",
    "arrival_trains = (\n",
    "    synthetic_perfect_user_with_change_train\n",
    "    .join(station_registry, on='STATION_ID', how='inner')\n",
    "    .filter(f.col('stop_order') != 1)\n",
    "    .withColumnRenamed('codice_treno', 'arrival_train_id')\n",
    "    .withColumnRenamed('stop_order', 'arrival_stop_order')\n",
    "    .withColumnRenamed('arrivo_teorico', 'arrival_train_arrival_time')\n",
    "    .withColumnRenamed('partenza_teorica', 'arrival_train_departure_time')\n",
    "    .select('STATION_ID', 'arrival_train_id', 'arrival_stop_order', 'arrival_train_arrival_time', 'arrival_train_departure_time','stazione')\n",
    ")\n",
    "\n",
    "arrival_departure_trains = (\n",
    "    departure_trains\n",
    "    .join(arrival_trains, on='STATION_ID', how='inner')\n",
    "    .filter(f.col('departure_stop_order') < f.col('arrival_stop_order'))\n",
    "    .filter(f.col('departure_train_id') != f.col('arrival_train_id'))\n",
    "    .withColumn('time_diff', (f.col('departure_train_departure_time') - f.col('arrival_train_arrival_time')) / 60)\n",
    "    .filter((f.col('time_diff') >= 5) & (f.col('time_diff') <= 60))\n",
    "    .select('station_id', 'departure_train_id', 'departure_stop_order', 'arrival_train_id', 'arrival_stop_order', 'time_diff')\n",
    "    .distinct()\n",
    "    # gemerate an id for each row\n",
    "    .withColumn('id', f.monotonically_increasing_id())\n",
    "    .withColumnRenamed('station_id', 'station_id_arrival_departure_synthetic_perfect_user_with_change_train')\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "change_train_journey = (\n",
    "    arrival_departure_trains\n",
    "    # join on synthetic_perfect_user_with_change_train with codice_treno and departure_train_id\n",
    "    .join(synthetic_perfect_user_with_change_train, (f.col('codice_treno') == f.col('departure_train_id')), how='inner')\n",
    "    .orderBy('id', 'arrivo_teorico')\n",
    "    .select('id','codice_treno','arrivo_teorico', 'partenza_teorica', 'stop_order','station_id', 'stazione')\n",
    "    .withColumn('change_train', f.lit(True))\n",
    ")\n",
    "\n",
    "pre_change_train_journey = (\n",
    "    arrival_departure_trains\n",
    "    # join on synthetic_perfect_user_with_change_train with codice_treno and arrival_train_id\n",
    "    .join(synthetic_perfect_user_with_change_train, (f.col('codice_treno') == f.col('arrival_train_id')), how='inner')\n",
    "    .orderBy('id', 'arrivo_teorico')\n",
    "    .select('id','codice_treno','arrivo_teorico', 'partenza_teorica', 'stop_order','station_id', 'stazione')\n",
    "    .withColumn('change_train', f.lit(False))\n",
    "    #.filter(f.col('stop_order') <= f.col('arrival_stop_order'))\n",
    ")\n",
    "\n",
    "# make a window over id \n",
    "window = Window.partitionBy('id').orderBy('arrivo_teorico')\n",
    "windo_without_order = Window.partitionBy('id')\n",
    "\n",
    "    \n",
    "synthetic_perfect_user_with_change_train = (\n",
    "    # concat change_train_journey and pre_change_train_journey\n",
    "    change_train_journey\n",
    "    .union(pre_change_train_journey)\n",
    "    .orderBy('id', 'arrivo_teorico')\n",
    "    # when the previous line is different from the current line for the chaang_train values modify the current line with the previous arrivo_teorico\n",
    "    .withColumn('partenza_teorica', f.when(f.lead('change_train').over(window) != f.col('change_train'), f.lead('partenza_teorica').over(window)).otherwise(f.col('partenza_teorica')))\n",
    "    # change station\n",
    "    .withColumn('change_station', f.when(f.lead('change_train').over(window) != f.col('change_train'), f.lead('station_id').over(window)).otherwise(None))\n",
    "    .withColumn('change_station', f.last('change_station', True).over(windo_without_order))\n",
    "    .filter(~((f.col('stop_order') == 1) & (f.col('change_train') == True)))\n",
    "    # take the last value of station_id for change_train value true over the window\n",
    "    .withColumn('stop_order_unified', f.row_number().over(window))\n",
    ")\n",
    "\n",
    "# Define windows specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"stop_order\")\n",
    "window_partition = Window.partitionBy(\"id\").orderBy(\"arrivo_teorico\")\n",
    "window_train = Window.partitionBy(\"id\",\"codice_treno\").orderBy(\"arrivo_teorico\")\n",
    "\n",
    "\n",
    "# Mark rows where station_id equals change_station\n",
    "synthetic_perfect_user_with_change_train = synthetic_perfect_user_with_change_train.withColumn(\"is_change_station\", f.col(\"station_id\") == f.col(\"change_station\"))\n",
    "\n",
    "# Create columns for rows before and after the change station\n",
    "synthetic_perfect_user_with_change_train = (\n",
    "    synthetic_perfect_user_with_change_train\n",
    "    .withColumn(\"row_idx\", f.row_number().over(window_spec))\n",
    "    .withColumn(\"lag_1\", f.lag(\"is_change_station\", 1).over(window_spec))\n",
    "    .withColumn(\"lag_2\", f.lag(\"is_change_station\", 2).over(window_spec))\n",
    "    .withColumn(\"lag_3\", f.lag(\"is_change_station\", 3).over(window_spec))\n",
    "    .withColumn(\"lag_4\", f.lag(\"is_change_station\", 4).over(window_spec))\n",
    "    .withColumn(\"lead_1\", f.lead(\"is_change_station\", 1).over(window_spec))\n",
    "    .withColumn(\"lead_2\", f.lead(\"is_change_station\", 2).over(window_spec))\n",
    "    .withColumn(\"lead_3\", f.lead(\"is_change_station\", 3).over(window_spec))\n",
    "    .withColumn(\"lead_4\", f.lead(\"is_change_station\", 4).over(window_spec))\n",
    ")\n",
    "\n",
    "# Filter the rows within the range of 4 rows before and after the change station\n",
    "filtered_df = synthetic_perfect_user_with_change_train.filter(\n",
    "    f.col(\"is_change_station\") |\n",
    "    f.col(\"lag_1\") | f.col(\"lag_2\") | f.col(\"lag_3\") | f.col(\"lag_4\") |\n",
    "    f.col(\"lead_1\") | f.col(\"lead_2\") | f.col(\"lead_3\") | f.col(\"lead_4\")\n",
    ").orderBy(\"id\", \"arrivo_teorico\")\n",
    "\n",
    "synthetic_perfect_user_with_change_train = (\n",
    "    filtered_df\n",
    "    # randomly delete a row with lag_1 == True\n",
    "    .withColumn(\"delete\", f.when(f.col(\"lag_1\"), f.rand() < 0.5).otherwise(False))\n",
    "    # randomly delete a row with lead_4 == True\n",
    "    .withColumn(\"delete\", f.when(f.col(\"lead_4\"), f.rand() < 0.5).otherwise(f.col(\"delete\")))\n",
    "    .filter(~f.col(\"delete\")) \n",
    "    .withColumn('origin', f.first('station_id').over(window_partition))\n",
    "    .withColumn('last_value', f.row_number().over(window_partition))\n",
    "    # use max last_value over the window partition to get the value of station_id\n",
    "    .withColumn('destination', f.when(f.col('last_value') == f.max('last_value').over(windo_without_order), f.col('station_id')).otherwise(None))\n",
    "    # fill the none value in the windo_without_order with the valid value of destination\n",
    "    .withColumn('destination', f.last('destination', True).over(windo_without_order))\n",
    "    .withColumn('OD_DATE_TIME_START', f.when(f.col('origin') == f.col('station_id'), f.col('arrivo_teorico')).otherwise(None))\n",
    "    .withColumn('OD_DATE_TIME_START', f.first('OD_DATE_TIME_START', True).over(windo_without_order))\n",
    "    .withColumn('OD_DATE_TIME_END', f.when(f.col('destination') == f.col('station_id'), f.col('partenza_teorica')).otherwise(None))\n",
    "    .withColumn('OD_DATE_TIME_END', f.last('OD_DATE_TIME_END', True).over(windo_without_order))\n",
    "    .distinct()\n",
    "    .filter(~((f.col('arrivo_teorico') == 0) & (f.col('partenza_teorica') == 0)))\n",
    "    .select('id', 'codice_treno', 'arrivo_teorico', 'partenza_teorica', 'station_id', 'stazione', 'change_train', 'change_station', 'stop_order_unified', 'origin', 'destination', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .filter((f.col('OD_DATE_TIME_START') != 0) & (f.col('OD_DATE_TIME_END') != 0))\n",
    "    .filter(f.col('origin') != f.col('destination'))\n",
    "    .withColumn('stop_order', f.row_number().over(window_partition))\n",
    "    # apply for each train_partition using a window a random delay\n",
    "    .withColumn('Rank', f.row_number().over(window_train))\n",
    "    .withColumn('delay', f.when(f.col('Rank') == 1, f.randn() * stddev + mean).otherwise(None))\n",
    "    .withColumn('delay', f.last('delay', True).over(window_train))\n",
    "    .withColumn('delay', f.when(f.col('delay') < 0, 0).when(f.col('delay') > 10, 10).otherwise(f.col('delay')))\n",
    "    .orderBy('id','arrivo_teorico','stop_order_unified')\n",
    "    .cache()\n",
    "\n",
    ")\n",
    "\n",
    "filtered_df\n",
    "synthetic_perfect_user_with_change_train.toPandas().to_csv('experiment_synthetic_user_ground_truth_with_change_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users with train transfer TEMPORAL NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth_with_change_train.csv', header=True)\n",
    "\n",
    "window_noiser = Window.partitionBy('id','codice_treno').orderBy(f.rand())\n",
    "\n",
    "sytnethic_user_input_stt_with_change = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('id', 'codice_treno', 'stop_order', 'DATE_TIME_START')  \n",
    "    # cast dealy column to int \n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    # add dealay to the DATE_TIME_START, dealys is in minutes and DATE_TIME_START is in seconds\n",
    "    .withColumn('DATE_TIME_START', f.col('DATE_TIME_START') + f.col('delay') * 60)\n",
    "    .withColumn('DATE_TIME_END', f.col('DATE_TIME_END') + f.col('delay') * 60)\n",
    "    .withColumn('DATE_TIME_START', f.col('DATE_TIME_START').cast('int'))\n",
    "    .withColumn('DATE_TIME_END', f.col('DATE_TIME_END').cast('int'))\n",
    "    .groupBy('DATE_ID', 'id', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    # if the origini is different from the first position of the list of stations and the destination has to be equal to the last position of the list of stations\n",
    "    .filter((f.col('ORIGIN') == f.col('STATIONS')[0]) & (f.col('DESTINATION') == f.col('STATIONS')[f.size(f.col('STATIONS')) - 1]))\n",
    "    # drop a random STATIONS in the list, only for stations with more than 2 stations, it doens't have to be the first or the last in the station list\n",
    "    .withColumn('STATIONS_LEN', f.size(f.col('STATIONS')))\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .filter((f.col('STATIONS_LEN') <= 8) & (f.col('STATIONS_LEN') > 3))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['id', 'ORIGIN', 'DESTINATION']) \n",
    "    .limit(10000)\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "sytnethic_user_input_stt_with_change.toPandas().to_csv('experiment_input_user_stt_synthetic_with_change_noise_temporal.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users with train transfer SPATIAL NOISE (stations removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth.csv', header=True)\n",
    "\n",
    "sytnethic_user_input_stt = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('IMSI', 'codice_treno', 'stop_order', 'date_time_start')\n",
    "    # cast delay column to int \n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    # add delay to the date_time_start, dealys is in minutes and date_time_start is in seconds\n",
    "    .groupBy('DATE_ID', 'IMSI', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    # if the origini is different from the first position of the list of stations and the destination has to be equal to the last position of the list of stations\n",
    "    .filter((f.col('ORIGIN') == f.col('STATIONS')[0]) & (f.col('DESTINATION') == f.col('STATIONS')[f.size(f.col('STATIONS')) - 1]))\n",
    "    # drop a random STATIONS in the list, only for stations with more than 2 stations, it doens't have to be the first or the last in the station list\n",
    "    .withColumn('STATIONS', f.when(f.size(f.col('STATIONS')) > 2, f.expr('transform(STATIONS, (x, i) -> if(i != 0 and i != size(STATIONS) - 1 and rand() > 0.5, null, x))')).otherwise(f.col('STATIONS')))\n",
    "    # find the index of the null values in the list of stations and drop the corresponding values in the list of dates\n",
    "    .withColumn('DATES_TIME_START', f.expr('transform(sequence(0, size(STATIONS) - 1), i -> if(STATIONS[i] is null, null, DATES_TIME_START[i]))'))\n",
    "    .withColumn('DATES_TIME_END', f.expr('transform(sequence(0, size(STATIONS) - 1), i -> if(STATIONS[i] is null, null, DATES_TIME_END[i]))'))\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['IMSI', 'ORIGIN', 'DESTINATION']) \n",
    "    .limit(10000)\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic_noise_only_spatial_(stations_removal).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users with train transfer SPATIAL NOISE (adding noise stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to insert noises\n",
    "def insert_noise_station(stations, idx, noise_flag):\n",
    "    if noise_flag and idx != 0:\n",
    "        random_station = -7\n",
    "        stations.insert(idx, random_station)\n",
    "    return stations\n",
    "\n",
    "def insert_noise_time(time, idx, noise_flag):\n",
    "    if noise_flag and idx != 0:\n",
    "        time_to_insert = int((time[idx - 1] + time[idx]) / 2)\n",
    "        time.insert(idx, time_to_insert)\n",
    "    return time\n",
    "\n",
    "# Register the UDF\n",
    "insert_noise_station_udf = f.udf(insert_noise_station, ArrayType(IntegerType()))\n",
    "insert_noise_time_udf = f.udf(insert_noise_time, ArrayType(IntegerType()))\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth_with_change_train.csv', header=True)\n",
    "\n",
    "# Process the DataFrame\n",
    "sytnethic_user_input_stt = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('id', 'codice_treno', 'stop_order', 'DATE_TIME_START')\n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    .withColumn('DATE_TIME_START', (f.col('DATE_TIME_START') + f.col('delay') * 60).cast('int'))\n",
    "    .withColumn('DATE_TIME_END', (f.col('DATE_TIME_END') + f.col('delay') * 60).cast('int'))\n",
    "    .groupBy('DATE_ID', 'id', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'), \n",
    "         f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'), \n",
    "         f.collect_list('DATE_TIME_END').alias('DATES_TIME_END')) \n",
    "    .withColumn('NOISE', f.rand() <= 0.5)\n",
    "    .withColumn('idx', f.floor(f.rand() * f.size(f.col('STATIONS'))).cast('int'))\n",
    "    # cast stations to array type of integer\n",
    "    .withColumn('STATIONS', f.col('STATIONS').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('STATIONS', insert_noise_station_udf(f.col('STATIONS'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('DATES_TIME_START', f.col('DATES_TIME_START').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('DATES_TIME_START', insert_noise_time_udf(f.col('DATES_TIME_START'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('DATES_TIME_END', f.col('DATES_TIME_END').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('DATES_TIME_END', insert_noise_time_udf(f.col('DATES_TIME_END'), f.col('idx'), f.col('NOISE'))) \n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END')) \n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .drop('NOISE', 'idx')\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# save the result\n",
    "sytnethic_user_input_stt.toPandas().to_csv('experiment_input_user_stt_synthetic_noise_only_spatial_(stations_noise_add).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE_ID</th><th>id</th><th>ORIGIN</th><th>DESTINATION</th><th>OD_DATE_TIME_START</th><th>OD_DATE_TIME_END</th><th>STATIONS</th><th>DATES_TIME_START</th><th>DATES_TIME_END</th><th>NOISE</th><th>idx</th></tr>\n",
       "<tr><td>20240702</td><td>1090921693194</td><td>818</td><td>1953</td><td>1696109760</td><td>1696118100</td><td>[829, 1394, 279, ...</td><td>[1696115700, 1696...</td><td>[1696115760, 1696...</td><td>false</td><td>6</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693216</td><td>1400</td><td>1452</td><td>1696083120</td><td>1696089900</td><td>[509, 1006, 1289,...</td><td>[1696087680, 1696...</td><td>[1696087740, 1696...</td><td>true</td><td>4</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693220</td><td>1953</td><td>909</td><td>1696096620</td><td>1696101300</td><td>[1953, 445, 909, ...</td><td>[1696096620, 1696...</td><td>[1696096980, 1696...</td><td>false</td><td>1</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693227</td><td>444</td><td>1273</td><td>1696095840</td><td>1696109580</td><td>[1625, 1645, 1103...</td><td>[1696096920, 1696...</td><td>[1696096980, 1696...</td><td>false</td><td>7</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693260</td><td>2005</td><td>1681</td><td>1696096320</td><td>1696098900</td><td>[1452, 1176, 1667...</td><td>[1696096920, 1696...</td><td>[1696097160, 1696...</td><td>false</td><td>5</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693262</td><td>1176</td><td>1681</td><td>1696097160</td><td>1696098900</td><td>[1176, 1667, 1681...</td><td>[1696097760, 1696...</td><td>[1696097820, 1696...</td><td>false</td><td>1</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693280</td><td>2006</td><td>2066</td><td>1696093860</td><td>1696103340</td><td>[836, 2066, 2006,...</td><td>[1696103400, 1696...</td><td>[1696103460, 1696...</td><td>true</td><td>2</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693281</td><td>1023</td><td>2066</td><td>1696092240</td><td>1696103340</td><td>[836, 2066, 1023,...</td><td>[1696102920, 1696...</td><td>[1696102980, 1696...</td><td>false</td><td>1</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693285</td><td>2024</td><td>1103</td><td>1696098660</td><td>1696105020</td><td>[1260, 1625, 1103...</td><td>[1696104240, 1696...</td><td>[1696104300, 1696...</td><td>true</td><td>1</td></tr>\n",
       "<tr><td>20240702</td><td>1090921693292</td><td>484</td><td>1103</td><td>1696102620</td><td>1696105020</td><td>[1645, 1103, 484,...</td><td>[1696105200, 1696...</td><td>[1696105500, 1696...</td><td>false</td><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+--------+-------------+------+-----------+------------------+----------------+--------------------+--------------------+--------------------+-----+---+\n",
       "| DATE_ID|           id|ORIGIN|DESTINATION|OD_DATE_TIME_START|OD_DATE_TIME_END|            STATIONS|    DATES_TIME_START|      DATES_TIME_END|NOISE|idx|\n",
       "+--------+-------------+------+-----------+------------------+----------------+--------------------+--------------------+--------------------+-----+---+\n",
       "|20240702|1090921693194|   818|       1953|        1696109760|      1696118100|[829, 1394, 279, ...|[1696115700, 1696...|[1696115760, 1696...|false|  6|\n",
       "|20240702|1090921693216|  1400|       1452|        1696083120|      1696089900|[509, 1006, 1289,...|[1696087680, 1696...|[1696087740, 1696...| true|  4|\n",
       "|20240702|1090921693220|  1953|        909|        1696096620|      1696101300|[1953, 445, 909, ...|[1696096620, 1696...|[1696096980, 1696...|false|  1|\n",
       "|20240702|1090921693227|   444|       1273|        1696095840|      1696109580|[1625, 1645, 1103...|[1696096920, 1696...|[1696096980, 1696...|false|  7|\n",
       "|20240702|1090921693260|  2005|       1681|        1696096320|      1696098900|[1452, 1176, 1667...|[1696096920, 1696...|[1696097160, 1696...|false|  5|\n",
       "|20240702|1090921693262|  1176|       1681|        1696097160|      1696098900|[1176, 1667, 1681...|[1696097760, 1696...|[1696097820, 1696...|false|  1|\n",
       "|20240702|1090921693280|  2006|       2066|        1696093860|      1696103340|[836, 2066, 2006,...|[1696103400, 1696...|[1696103460, 1696...| true|  2|\n",
       "|20240702|1090921693281|  1023|       2066|        1696092240|      1696103340|[836, 2066, 1023,...|[1696102920, 1696...|[1696102980, 1696...|false|  1|\n",
       "|20240702|1090921693285|  2024|       1103|        1696098660|      1696105020|[1260, 1625, 1103...|[1696104240, 1696...|[1696104300, 1696...| true|  1|\n",
       "|20240702|1090921693292|   484|       1103|        1696102620|      1696105020|[1645, 1103, 484,...|[1696105200, 1696...|[1696105500, 1696...|false|  0|\n",
       "+--------+-------------+------+-----------+------------------+----------------+--------------------+--------------------+--------------------+-----+---+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sytnethic_user_input_stt_combined_noise = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('id', 'codice_treno', 'stop_order', 'DATE_TIME_START')\n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    .withColumn('DATE_TIME_START', (f.col('DATE_TIME_START') + f.col('delay') * 60).cast('int'))\n",
    "    .withColumn('DATE_TIME_END', (f.col('DATE_TIME_END') + f.col('delay') * 60).cast('int'))\n",
    "    .groupBy('DATE_ID', 'id', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'),\n",
    "         f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'),\n",
    "         f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    .withColumn('NOISE', f.rand() <= 0.5)\n",
    "    .withColumn('idx', f.floor(f.rand() * f.size(f.col('STATIONS'))).cast('int'))\n",
    ")\n",
    "sytnethic_user_input_stt_combined_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic users with train transfer COMBINED NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Read the ground truth data\n",
    "synthetic_user = spark.read.csv('./experiment_synthetic_user_ground_truth_with_change_train.csv', header=True)\n",
    "\n",
    "# Define the function to insert noise stations and times\n",
    "def insert_noise_station(stations, idx, noise_flag):\n",
    "    if noise_flag and idx != 0 and idx < len(stations):\n",
    "        random_station = -7\n",
    "        stations.insert(idx, random_station)\n",
    "    return stations\n",
    "\n",
    "def insert_noise_time(time, idx, noise_flag):\n",
    "    if noise_flag and idx != 0 and idx < len(time):\n",
    "        time_to_insert = int((time[idx - 1] + time[idx]) / 2)\n",
    "        time.insert(idx, time_to_insert)\n",
    "    return time\n",
    "\n",
    "# Register the UDF\n",
    "insert_noise_station_udf = f.udf(insert_noise_station, ArrayType(IntegerType()))\n",
    "insert_noise_time_udf = f.udf(insert_noise_time, ArrayType(IntegerType()))\n",
    "\n",
    "# Window specifications\n",
    "window_noiser = Window.partitionBy('id', 'codice_treno').orderBy(f.rand())\n",
    "window_partition = Window.partitionBy(\"id\").orderBy(\"arrivo_teorico\")\n",
    "\n",
    "# Create synthetic users with combined noise\n",
    "sytnethic_user_input_stt_combined_noise = (\n",
    "    synthetic_user\n",
    "    .withColumn('DATE_ID', f.date_format(f.to_date(f.lit(datetime.datetime.now())), 'yyyyMMdd'))\n",
    "    .withColumnRenamed('station_id', 'STATION')\n",
    "    .withColumnRenamed('arrivo_teorico', 'DATE_TIME_START')\n",
    "    .withColumnRenamed('partenza_teorica', 'DATE_TIME_END')\n",
    "    .orderBy('id', 'codice_treno', 'stop_order', 'DATE_TIME_START')\n",
    "    .withColumn('delay', f.col('delay').cast('int'))\n",
    "    .withColumn('DATE_TIME_START', (f.col('DATE_TIME_START') + f.col('delay') * 60).cast('int'))\n",
    "    .withColumn('DATE_TIME_END', (f.col('DATE_TIME_END') + f.col('delay') * 60).cast('int'))\n",
    "    .groupBy('DATE_ID', 'id', 'ORIGIN', 'DESTINATION', 'OD_DATE_TIME_START', 'OD_DATE_TIME_END')\n",
    "    .agg(f.collect_list('STATION').alias('STATIONS'),\n",
    "         f.collect_list('DATE_TIME_START').alias('DATES_TIME_START'),\n",
    "         f.collect_list('DATE_TIME_END').alias('DATES_TIME_END'))\n",
    "    .withColumn('NOISE', f.rand() <= 0.5)\n",
    "    .withColumn('idx', f.floor(f.rand() * f.size(f.col('STATIONS'))).cast('int'))\n",
    "    .withColumn('STATIONS', f.col('STATIONS').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('STATIONS', insert_noise_station_udf(f.col('STATIONS'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('DATES_TIME_START', f.col('DATES_TIME_START').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('DATES_TIME_START', insert_noise_time_udf(f.col('DATES_TIME_START'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('DATES_TIME_END', f.col('DATES_TIME_END').cast(ArrayType(IntegerType())))\n",
    "    .withColumn('DATES_TIME_END', insert_noise_time_udf(f.col('DATES_TIME_END'), f.col('idx'), f.col('NOISE')))\n",
    "    .withColumn('STATIONS', f.concat_ws('|', 'STATIONS'))\n",
    "    .withColumn('DATES_TIME_START', f.concat_ws('|', 'DATES_TIME_START'))\n",
    "    .withColumn('DATES_TIME_END', f.concat_ws('|', 'DATES_TIME_END'))\n",
    "    .withColumnRenamed('STATIONS', 'STATIONS_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_START', 'DATES_TIME_START_ALL')\n",
    "    .withColumnRenamed('DATES_TIME_END', 'DATES_TIME_END_ALL')\n",
    "    .dropDuplicates(['id', 'ORIGIN', 'DESTINATION'])\n",
    "    .limit(10000)\n",
    "    .repartition(1)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Save the result to CSV\n",
    "sytnethic_user_input_stt_combined_noise.toPandas().to_csv('experiment_input_user_stt_synthetic_with_change_noise_combined.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
